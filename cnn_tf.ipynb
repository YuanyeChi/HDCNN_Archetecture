{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hierarchically Deep Convolutional Neural Network For Architecture Image Recognition"
      ],
      "metadata": {
        "id": "WREb-GwCDjON"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and Imports"
      ],
      "metadata": {
        "id": "sIMFIbzkDjOP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Packages**"
      ],
      "metadata": {
        "id": "bK2YfMDWDjOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras as kr\n",
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from random import randint\n",
        "import time\n",
        "import os"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "WARNING:tensorflow:From /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\nInstructions for updating:\nnon-resource variables are not supported in the long term\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "id": "o8jkSNBTDjOQ",
        "gather": {
          "logged": 1651982462598
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('models/tf'):\n",
        "    os.mkdir('models/tf')"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "id": "cIywIdtKDjOR",
        "gather": {
          "logged": 1651982462925
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Global Variables**"
      ],
      "metadata": {
        "id": "OcpoTSj4DjOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The number of coarse categories\n",
        "coarse_categories = 9\n",
        "\n",
        "# The number of fine categories\n",
        "fine_categories = 25"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "id": "m00DWErIDjOS",
        "gather": {
          "logged": 1651982463046
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import and Preprocess Dataset"
      ],
      "metadata": {
        "id": "gr7TENfZDjOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import keras_preprocessing.image\n",
        "\n",
        "def load_and_crop_img(path, grayscale=False, color_mode='rgb', target_size=None,\n",
        "             interpolation='nearest'):\n",
        "    \"\"\"\n",
        "    Wraps keras_preprocessing.image.utils.loag_img() and adds cropping.\n",
        "    Cropping method enumarated in interpolation\n",
        "    # Arguments\n",
        "        path: Path to image file.\n",
        "        color_mode: One of \"grayscale\", \"rgb\", \"rgba\". Default: \"rgb\".\n",
        "            The desired image format.\n",
        "        target_size: Either `None` (default to original size)\n",
        "            or tuple of ints `(img_height, img_width)`.\n",
        "        interpolation: Interpolation and crop methods used to resample and crop the image\n",
        "            if the target size is different from that of the loaded image.\n",
        "            Methods are delimited by \":\" where first part is interpolation and second is crop\n",
        "            e.g. \"lanczos:random\".\n",
        "            Supported interpolation methods are \"nearest\", \"bilinear\", \"bicubic\", \"lanczos\",\n",
        "            \"box\", \"hamming\" By default, \"nearest\" is used.\n",
        "            Supported crop methods are \"none\", \"center\", \"random\".\n",
        "    # Returns\n",
        "        A PIL Image instance.\n",
        "    # Raises\n",
        "        ImportError: if PIL is not available.\n",
        "        ValueError: if interpolation method is not supported.\n",
        "    \"\"\"\n",
        "\n",
        "    # Decode interpolation string. Allowed Crop methods: none, center, random\n",
        "    interpolation, crop = interpolation.split(\":\") if \":\" in interpolation else (interpolation, \"none\")  \n",
        "\n",
        "    if crop == \"none\":\n",
        "        return keras_preprocessing.image.utils.load_img(path, \n",
        "                                            grayscale=grayscale, \n",
        "                                            color_mode=color_mode, \n",
        "                                            target_size=target_size,\n",
        "                                            interpolation=interpolation)\n",
        "\n",
        "    # Load original size image using Keras\n",
        "    img = keras_preprocessing.image.utils.load_img(path, \n",
        "                                            grayscale=grayscale, \n",
        "                                            color_mode=color_mode, \n",
        "                                            target_size=None, \n",
        "                                            interpolation=interpolation)\n",
        "\n",
        "    # Crop fraction of total image\n",
        "    crop_fraction = 0.875\n",
        "    target_width = target_size[1]\n",
        "    target_height = target_size[0]\n",
        "\n",
        "    if target_size is not None:        \n",
        "        if img.size != (target_width, target_height):\n",
        "\n",
        "            if crop not in [\"center\", \"random\"]:\n",
        "                raise ValueError('Invalid crop method {} specified.', crop)\n",
        "\n",
        "            if interpolation not in keras_preprocessing.image.utils._PIL_INTERPOLATION_METHODS:\n",
        "                raise ValueError(\n",
        "                    'Invalid interpolation method {} specified. Supported '\n",
        "                    'methods are {}'.format(interpolation,\n",
        "                        \", \".join(keras_preprocessing.image.utils._PIL_INTERPOLATION_METHODS.keys())))\n",
        "            \n",
        "            resample = keras_preprocessing.image.utils._PIL_INTERPOLATION_METHODS[interpolation]\n",
        "\n",
        "            width, height = img.size\n",
        "\n",
        "            # Resize keeping aspect ratio\n",
        "            # result shold be no smaller than the targer size, include crop fraction overhead\n",
        "            target_size_before_crop = (target_width/crop_fraction, target_height/crop_fraction)\n",
        "            ratio = max(target_size_before_crop[0] / width, target_size_before_crop[1] / height)\n",
        "            target_size_before_crop_keep_ratio = int(width * ratio), int(height * ratio)\n",
        "            img = img.resize(target_size_before_crop_keep_ratio, resample=resample)\n",
        "\n",
        "            width, height = img.size\n",
        "\n",
        "            if crop == \"center\":\n",
        "                left_corner = int(round(width/2)) - int(round(target_width/2))\n",
        "                top_corner = int(round(height/2)) - int(round(target_height/2))\n",
        "                return img.crop((left_corner, top_corner, left_corner + target_width, top_corner + target_height))\n",
        "            elif crop == \"random\":\n",
        "                left_shift = random.randint(0, int((width - target_width)))\n",
        "                down_shift = random.randint(0, int((height - target_height)))\n",
        "                return img.crop((left_shift, down_shift, target_width + left_shift, target_height + down_shift))\n",
        "\n",
        "    return img\n",
        "  \n",
        "keras_preprocessing.image.iterator.load_img = load_and_crop_img"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "id": "VorDvH9A5TlK",
        "gather": {
          "logged": 1651982463179
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "\n",
        "fine_datagen = tf.keras.preprocessing.image.ImageDataGenerator(    \n",
        "    rotation_range=20,\n",
        "    horizontal_flip=True,\n",
        "    samplewise_std_normalization = True)\n",
        "fine_dir = \"dataset\"\n",
        "fine_img = fine_datagen.flow_from_directory(fine_dir,target_size=(64,64), batch_size=128, interpolation = 'lanczos:center')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/keras_preprocessing/image/image_data_generator.py:356: UserWarning: This ImageDataGenerator specifies `samplewise_std_normalization`, which overrides setting of `samplewise_center`.\n  warnings.warn('This ImageDataGenerator specifies '\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Found 10113 images belonging to 2 classes.\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmgxfscyDJy7",
        "outputId": "5b615974-4df8-4521-b42e-8376c1b5ab23",
        "gather": {
          "logged": 1651982464448
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "S4bkFOSmZe-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "fine_img.reset()\n",
        "X_fine, y_fine = next(fine_img)\n",
        "for i in tqdm(range(len(fine_img))-1): # 1st batch is already fetched before the for loop.\n",
        "  img, label = next(fine_img)\n",
        "  X_fine = np.append(X_fine, img, axis=0)\n",
        "  y_fine = np.append(y_fine, label, axis=0)\n",
        "print(X_fine.shape, y_fine.shape)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "100%|██████████| 79/79 [12:27<00:00,  9.46s/it]\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gD4-pUyc8xKs",
        "outputId": "c76fd723-6912-455a-eb14-5b8de8df97b1",
        "gather": {
          "logged": 1651983225139
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split Training set into Training and Validation sets**"
      ],
      "metadata": {
        "id": "33-wnYgdDjOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_val, y_train, y_val = train_test_split(X_fine, y_fine, test_size=.2,shuffle=True)"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "id": "elDqccurDjOZ",
        "gather": {
          "logged": 1651983368036
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# from IPython.core.display import display, HTML\n",
        "\n",
        "# for i in range(0,10):\n",
        "#     image = x_train[i]\n",
        "#     plt.imshow(image)\n",
        "#     plt.show()\n",
        "#     print(np.where(y_train[i] == 1)[0])\n",
        "\n",
        "\n",
        "# for i in range(0,10):\n",
        "#     image = x_val[i]\n",
        "#     plt.imshow(image)\n",
        "#     plt.show()\n",
        "#     print(np.where(y_train[i] == 1)[0])"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WTmCiLQwjVx1",
        "outputId": "7867d56d-7f12-414b-bd30-cae187ac5a3a",
        "gather": {
          "logged": 1651983225580
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# y_fine_eight = y_fine[np.argwhere(y_fine==1)[:,1] < 8]\n",
        "# X_fine_eight = X_fine[np.argwhere(y_fine==1)[:,1] < 8]\n",
        "# x_train_eight, x_val_eight, y_train_eight, y_val_eight = train_test_split(X_fine_eight, y_fine_eight, test_size=.1,shuffle=True)"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "id": "GsYVTQhoVnrF",
        "gather": {
          "logged": 1651983225702
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Constructing CNN**"
      ],
      "metadata": {
        "id": "8oUI-QnFDjOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import optimizers\n",
        "from keras.layers import Input, Conv2D, Dropout, MaxPooling2D, Flatten, Dense, BatchNormalization\n",
        "from keras.models import Model\n",
        "\n",
        "in_layer = Input(shape=(64, 64, 3), dtype='float32', name='main_input')\n",
        "\n",
        "net = Conv2D(64, 3, strides=1, padding='same', activation='relu')(in_layer)\n",
        "net = BatchNormalization()(net)\n",
        "net = MaxPooling2D((2, 2), padding='valid')(net)\n",
        "\n",
        "net = Conv2D(128, 3, strides=1, padding='same', activation='relu')(net)\n",
        "net = BatchNormalization()(net)\n",
        "# net = Dropout(.3)(net)\n",
        "net = MaxPooling2D((2, 2), padding='valid')(net)\n",
        "\n",
        "net = Conv2D(256, 3, strides=1, padding='same', activation='relu')(net)\n",
        "net = BatchNormalization()(net)\n",
        "# net = Dropout(.4)(net)\n",
        "net = MaxPooling2D((2, 2), padding='valid')(net)\n",
        "\n",
        "net = Conv2D(512, 3, strides=1, padding='same', activation='relu')(net)\n",
        "net = BatchNormalization()(net)\n",
        "# net = Dropout(.5)(net)\n",
        "net = MaxPooling2D((2, 2), padding='valid')(net)\n",
        "\n",
        "net = Flatten()(net)\n",
        "\n",
        "net = Dense(1024, activation='relu')(net)\n",
        "net = BatchNormalization()(net)\n",
        "net = Dropout(.3)(net)\n",
        "net = Dense(25, activation='softmax')(net)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "WARNING:tensorflow:From /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/keras/layers/normalization/batch_normalization.py:532: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "id": "ILCBmsHkDjOc",
        "gather": {
          "logged": 1651983226233
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compile Model**"
      ],
      "metadata": {
        "id": "_2rtuFv9DjOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(inputs=in_layer,outputs=net)\n",
        "adam_coarse = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer= adam_coarse, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "tbCallBack = kr.callbacks.TensorBoard(log_dir='./data/graph/relu_drop/', histogram_freq=0, write_graph=True, write_images=True)"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "id": "xJsd6YQ6DjOc",
        "gather": {
          "logged": 1651983226570
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train Model**"
      ],
      "metadata": {
        "id": "iVJdJDduDjOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch = 32"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "id": "0u_oZwXndb4R",
        "gather": {
          "logged": 1651983226692
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index= 0\n",
        "step = 5\n",
        "stop = 20\n",
        "\n",
        "\n",
        "while index < stop:\n",
        "    model.fit(x_train, y_train, batch_size=batch, initial_epoch=index, epochs=index+step, validation_data=(x_val, y_val), callbacks=[tbCallBack])\n",
        "    index += step\n",
        "    model.save_weights('models/tf/model_coarse'+str(index))\n",
        "\n",
        "save_index = index"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "A target array with shape (8090, 2) was passed for an output of shape (None, 25) while using as loss `categorical_crossentropy`. This loss expects targets to have the same shape as the output.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-99d1a71ce51a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtbCallBack\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mindex\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models/tf/model_coarse'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_select_training_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m     return func.fit(\n\u001b[0m\u001b[1;32m    778\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/keras/engine/training_arrays_v1.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    614\u001b[0m                                                      steps_per_epoch, x)\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m     x, y, sample_weights = model._standardize_user_data(\n\u001b[0m\u001b[1;32m    617\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/keras/engine/training_v1.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2333\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2335\u001b[0;31m     return self._standardize_tensors(\n\u001b[0m\u001b[1;32m   2336\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2337\u001b[0m         \u001b[0mrun_eagerly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/keras/engine/training_v1.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[0;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   2444\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrun_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2445\u001b[0m           \u001b[0;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2446\u001b[0;31m           training_utils_v1.check_loss_and_target_compatibility(\n\u001b[0m\u001b[1;32m   2447\u001b[0m               y, self._feed_loss_fns, feed_output_shapes)\n\u001b[1;32m   2448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/keras/engine/training_utils_v1.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[0;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mloss_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_loss_wrapper\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m             \u001b[0mloss_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m           raise ValueError('A target array with shape ' + str(y.shape) +\n\u001b[0m\u001b[1;32m    812\u001b[0m                            \u001b[0;34m' was passed for an output of shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m                            \u001b[0;34m' while using as loss `'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: A target array with shape (8090, 2) was passed for an output of shape (None, 25) while using as loss `categorical_crossentropy`. This loss expects targets to have the same shape as the output."
          ]
        }
      ],
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y47GeFu4DjOd",
        "outputId": "d88f5098-441b-4163-f54a-32ff7f622d42",
        "gather": {
          "logged": 1651983636754
        }
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "HDCNN_final.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "azureml_py38_pt_tf",
      "language": "python",
      "display_name": "Python 3.8 - Pytorch and Tensorflow"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "toc": {
      "number_sections": true,
      "toc_window_display": false,
      "threshold": 4,
      "navigate_menu": true,
      "colors": {
        "hover_highlight": "#DAA520",
        "running_highlight": "#FF0000",
        "selected_highlight": "#FFD700"
      },
      "toc_section_display": "block",
      "widenNotebook": false,
      "moveMenuLeft": true,
      "toc_cell": false,
      "nav_menu": {
        "height": "12px",
        "width": "252px"
      },
      "sideBar": true
    },
    "kernel_info": {
      "name": "azureml_py38_pt_tf"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}